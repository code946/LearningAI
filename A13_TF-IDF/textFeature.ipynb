{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《按图索骥学机器学习》-《A13_TF-IDF》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是《按图索骥学机器学习》-《A13_TF-IDF》的讲义\n",
    "这门课程之所以叫按图索骥，是因为学习资料都放到了思维导图当中，大家可以根据自己的情况，选择合适的学习路径，自主学习\n",
    "\n",
    "![avatar](pic/swnt.png)\n",
    "\n",
    "导图和有关学习资料都放在了github(git.code946.com)上，并且在不断迭代和更新中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"用来研究人工智能的主要物质基础以及能够实现人工智能技术平台的机器就是计算机，人工智能的发展历史是和计算机科学技术的发展史联系在一起的。\"\n",
    "str2 = \"机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\"\n",
    "str3 = \"目前，传统机器学习的研究方向主要包括决策树、随机森林、人工神经网络、贝叶斯学习等方面的研究\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = jieba.cut(str1)\n",
    "txt2 = jieba.cut(str2)\n",
    "txt3 = jieba.cut(str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\" \".join(txt1),\" \".join(txt2),\" \".join(txt3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['用来 研究 人工智能 的 主要 物质基础 以及 能够 实现 人工智能 技术 平台 的 机器 就是 计算机 ， 人工智能 的 发展 历史 是 和 计算机 科学技术 的 发展史 联系 在 一起 的 。',\n",
       " '机器 学习 是 一门 多 领域 交叉学科 ， 涉及 概率论 、 统计学 、 逼近 论 、 凸 分析 、 算法 复杂度 理论 等 多门 学科 。 专门 研究 计算机 怎样 模拟 或 实现 人类 的 学习 行为 ， 以 获取 新 的 知识 或 技能 ， 重新 组织 已有 的 知识结构 使 之 不断 改善 自身 的 性能 。',\n",
       " '目前 ， 传统 机器 学习 的 研究 方向 主要 包括 决策树 、 随机 森林 、 人工神经网络 、 贝叶斯 学习 等 方面 的 研究']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [line.strip() for line in open('stop_words_zh.txt', 'r', encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一起', '一门', '不断', '专门', '主要', '交叉学科', '人工智能', '人工神经网络', '人类', '传统', '决策树', '分析', '包括', '历史', '发展', '发展史', '复杂度', '多门', '学习', '学科', '实现', '已有', '平台', '性能', '技术', '技能', '改善', '方向', '方面', '机器', '森林', '概率论', '模拟', '涉及', '物质基础', '理论', '用来', '目前', '知识', '知识结构', '研究', '科学技术', '算法', '组织', '统计学', '联系', '能够', '获取', '行为', '计算机', '贝叶斯', '逼近', '重新', '随机', '领域']\n",
      "[[1 0 0 0 1 0 3 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0\n",
      "  1 0 0 0 1 1 0 0 0 1 1 0 0 2 0 0 0 0 0]\n",
      " [0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 2 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1\n",
      "  0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1]\n",
      " [0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0\n",
      "  0 1 0 0 2 0 0 0 0 0 0 0 0 0 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vector = CountVectorizer(stop_words=stopwords)\n",
    "res = vector.fit_transform(content)\n",
    "print(vector.get_feature_names())\n",
    "print(res.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一起', '一门', '不断', '专门', '主要', '交叉学科', '人工智能', '人工神经网络', '人类', '传统', '决策树', '分析', '包括', '历史', '发展', '发展史', '复杂度', '多门', '学习', '学科', '实现', '已有', '平台', '性能', '技术', '技能', '改善', '方向', '方面', '机器', '森林', '概率论', '模拟', '涉及', '物质基础', '理论', '用来', '目前', '知识', '知识结构', '研究', '科学技术', '算法', '组织', '统计学', '联系', '能够', '获取', '行为', '计算机', '贝叶斯', '逼近', '重新', '随机', '领域']\n",
      "[[0.20341322 0.         0.         0.         0.154701   0.\n",
      "  0.61023967 0.         0.         0.         0.         0.\n",
      "  0.         0.20341322 0.20341322 0.20341322 0.         0.\n",
      "  0.         0.         0.154701   0.         0.20341322 0.\n",
      "  0.20341322 0.         0.         0.         0.         0.12013913\n",
      "  0.         0.         0.         0.         0.20341322 0.\n",
      "  0.20341322 0.         0.         0.         0.12013913 0.20341322\n",
      "  0.         0.         0.         0.20341322 0.20341322 0.\n",
      "  0.         0.309402   0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.17912045 0.17912045 0.17912045 0.         0.17912045\n",
      "  0.         0.         0.17912045 0.         0.         0.17912045\n",
      "  0.         0.         0.         0.         0.17912045 0.17912045\n",
      "  0.27245144 0.17912045 0.13622572 0.17912045 0.         0.17912045\n",
      "  0.         0.17912045 0.17912045 0.         0.         0.10579142\n",
      "  0.         0.17912045 0.17912045 0.17912045 0.         0.17912045\n",
      "  0.         0.         0.17912045 0.17912045 0.10579142 0.\n",
      "  0.17912045 0.17912045 0.17912045 0.         0.         0.17912045\n",
      "  0.17912045 0.13622572 0.         0.17912045 0.17912045 0.\n",
      "  0.17912045]\n",
      " [0.         0.         0.         0.         0.19879284 0.\n",
      "  0.         0.2613887  0.         0.2613887  0.2613887  0.\n",
      "  0.2613887  0.         0.         0.         0.         0.\n",
      "  0.39758569 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.2613887  0.2613887  0.15438038\n",
      "  0.2613887  0.         0.         0.         0.         0.\n",
      "  0.         0.2613887  0.         0.         0.30876075 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.2613887  0.         0.         0.2613887\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(stop_words=stopwords)\n",
    "res = tf.fit_transform(content)\n",
    "print(tf.get_feature_names())\n",
    "print(res.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
